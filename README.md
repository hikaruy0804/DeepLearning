# プログラムで深層学習の基礎を理解する。

深層学習の基本的な仕組みや学習推論のプロセスについてプログラミング上でコードを実行しながら理解します。

### 目的

- 深層学習モデル構造の基本を理解する。
- できることやできないことがイメージできるようになる。
- 自分で深層学習を学んでいくきっかけにする。
- 実装のヒントやきっかけにする。

以降で事前知識やライブラリについて説明しています。
コード内でも説明を記載しているので、一度流し読みをしてコード実行時に必要に応じて読み返してください。

---

### コード

[1.深層学習（全結合層）](https://colab.research.google.com/github/hikaruy0804/DeepLearning/blob/main/1_%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92%EF%BC%88%E5%85%A8%E7%B5%90%E5%90%88%E5%B1%A4%EF%BC%89.ipynb)
[2-1.深層学習（畳み込み-CPU）](https://colab.research.google.com/github/hikaruy0804/DeepLearning/blob/main/2_1_%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92%EF%BC%88%E7%95%B3%E3%81%BF%E8%BE%BC%E3%81%BF_CPU%EF%BC%89.ipynb)
[2-2.深層学習（畳み込み-GPU）](https://colab.research.google.com/github/hikaruy0804/DeepLearning/blob/main/2_2_%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92%EF%BC%88%E7%95%B3%E3%81%BF%E8%BE%BC%E3%81%BF_GPU%EF%BC%89.ipynb)
[3.深層学習（転移学習モデル）](https://colab.research.google.com/github/hikaruy0804/DeepLearning/blob/main/3_%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92%EF%BC%88%E8%BB%A2%E7%A7%BB%E5%AD%A6%E7%BF%92%E3%83%A2%E3%83%87%E3%83%AB%EF%BC%89.ipynb)

---

はじめに、AI・機械学習・深層学習の違いを理解しておきましょう。
![AI](https://github.com/HAGAKURE-PROGRAMMING-ACADEMY/KEIJIBAN/assets/88568970/9f1642fe-0ff3-4e49-94c9-767970337fad)

### 深層学習

機械学習の一種で、人工的に構築された多層のニューラルネットワークを用いて、データから複雑なパターンを学習させる手法です。 深層学習は、データ自体からパターンを学習することで、高い精度での予測や分類を行うことができます。画像認識や音声認識、自然言語処理などの分野で活用されています。多数の層からなるニューラルネットワークを訓練するために大量のデータと計算リソースが必要になります。

### 深層学習のジャンルとタスク

ざっくり分類すると3ジャンル、2タスクがあります。

1. 画像（動画）
    - 認識（識別）: 物体認識、顔認識、感情分析、動作認識など。
    - 生成: 画像生成、画像変換、動画生成など。
2. テキスト
    - 認識（識別）: 自然言語処理、感情分析、テキスト分類など。
    - 生成: 文章生成、翻訳など。
3. 音声
    - 認識（識別）: 音声認識、話者識別など。
    - 生成**: 音声合成、音楽生成、音声変換など。

画像識別を例にGoogleColabでコードを実行しながら3つ学びます。
※今回は生成モデルには触れません。

- 全結合層
- 畳み込み層
- 転移学習

データを準備し深層学習モデルが学習して性能を出すところまでを確認します。

1. 学習用データの準備
2. ニューラルネットワークモデルの構築・実行
3. 精度の確認
4. 簡単な演習

---

### PyTorchライブラリ

今回はFacebookが開発した深層学習ライブラPyTorchを使用します。
他にも深層学習ライブラリはありますが、PyTorchのコードを見る割合が多いと感じる（し、自分がよく使う）ので、今回はPyTorchを使用します。

#### 他の深層学習ライブラリ

- TensorFlow: Googleが開発したオープンソースの深層学習フレームワーク
- Keras: TensorFlowやTheanoをバックエンドとした高水準のAPIを提供する深層学習フレームワーク
- Caffe: Berkeley Vision and Learning Centerによって開発された深層学習フレームワーク
- Chainer: Preferred Networksが開発した深層学習フレームワーク

### データセット

深層学習では、学習用データの準備が最も重要ですがとても労力がかかります。また、モデルの精度を確かめるには同一のデータを使うことで精度を比較することができます。
PyTorchには「torchvision」というパッケージがあり画像データの読み込みや変換を行う機能があります。CIFAR10データセットを「torchvision」で読み込んで深層学習における学習や推論に使用します。

**CIFAR-10**: 飛行機、自動車、鳥、猫、鹿、犬、カエル、馬、船、トラックの10種類の画像が含まれています。これらの画像は、3つの色チャネル（赤、緑、青）を持つ32×32ピクセルの画像です。

イメージ画像

#### 他のデータセット

深層学習において使用される代表的なデータセット
1. CIFAR-10: カナダのトロント大学が公開している10種類のカテゴリー（飛行機、自動車、鳥、猫、鹿、犬、カエル、馬、船、トラック）の小さなカラー画像（32×32ピクセル）60,000枚を含むデータセットです。これは、画像認識タスクに広く使用されています。
2. CIFAR-100: CIFAR-10と同様に、60,000枚の32×32ピクセルの小さなカラー画像を含むデータセットです。100種類のカテゴリーに分けられており、各カテゴリーには600枚の画像が含まれています。これにより、より複雑で詳細な画像認識や分類のタスクに使用されます。
3. MNIST: 手書き数字（0から9まで）の70,000枚のグレースケール画像（28×28ピクセル）を含むデータセットで、画像処理やパターン認識の分野での基本的なベンチマークとして使われます。
4. ImageNet: これは、1,000以上のカテゴリーに分類される1400万枚以上の画像を含む大規模なデータセットです。画像認識、分類、検出タスクに広く使用されています。
6. COCO（Common Objects in Context）: このデータセットは、物体検出、セグメンテーション、キャプション生成などに使用される、様々な環境で撮影された画像を含んでいます。人、動物、交通手段など多様な物体が含まれています。

### torchvision

torchvisionの`transforms`ライブラリで設定する（設定できる）処理
1. ToTensor: この変換は非常に一般的で、画像をPIL形式またはNumPy配列からPyTorchのテンソルに変換します。テンソルは、PyTorchでの画像処理とモデルトレーニングにおいて標準的な形式です。
2. Normalize: 画像データのピクセル値を正規化するために使用されます。通常、チャネルごとの平均と標準偏差を指定します。これにより、モデルのトレーニングが容易になり、収束速度が向上することが多いです。
3. Resize/CenterCrop/RandomCrop: これらの変換は、画像のサイズを変更したり、中央またはランダムな位置で切り取ったりします。これにより、異なるサイズの画像を一定のサイズに揃え、ネットワークに入力できるようにします。
4. RandomHorizontalFlip/RandomVerticalFlip: これらはデータ拡張のために使われることが多く、ランダムに画像を水平または垂直に反転させます。これにより、モデルの汎用性が向上し、過学習を防ぐ効果があります。
5. ColorJitter: 画像の明るさ、コントラスト、飽和度、色相をランダムに変更して、より多様なデータセットを生成します。これもまた、データ拡張の一環として有効です。

### 正規化

深層学習モデルに画像を学習させるには画像を数値に変換する必要があります。画像の各ピクセルは0から255の範囲の整数値で表されます。しかし、深層学習の処理を効率的に行うためにはさらに変換する必要があります。

この変換プロセスを正規化と呼びます。

[0, 255]から[0, 1]から[-1, 1] へ正規化（変換）する理由
1. 入力データの中心化: データを [-1, 1] の範囲に正規化することで、入力データの平均が0に近づきます。これは「中心化」と呼ばれ、ネットワークの学習を安定させ勾配降下法による最適化がより効率的になる傾向があります。
2. 活性化関数: ReLU（Rectified Linear Unit）やその変種が活性化関数として使用されます。ReLUは正の入力に対しては線形であり、負の入力に対しては0を返します。入力データが [-1, 1] の範囲にある場合、ネットワークは負と正の両方の入力値を効果的に処理でき、ReLUの非対称性がモデルの学習に役立ちます。
3. 特徴の重み付けの均一化: データを [-1, 1] に正規化することで、異なる特徴量間のスケールが均一化され特徴間のバランスをより適切に学習するのに役立ちます。
4. 学習の収束速度の向上: データの範囲を [-1, 1] にすると、重みの初期化や勾配の更新が改善され、モデルの収束が速くなることが多いです。
5. 事前学習されたモデルとの互換性: 事前学習されたモデルは、入力データが [-1, 1] の範囲にあることを前提としている。

